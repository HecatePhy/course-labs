{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project 1 level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 1 : pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "import pickle\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "# word segmentation\n",
    "label2id = {}\n",
    "def split_words(dataset):\n",
    "    raw_docs = []\n",
    "    docs = []\n",
    "    labels = []\n",
    "    for topic, datas in dataset.items():\n",
    "        if not topic in label2id.keys():\n",
    "            label2id[topic] = len(list(label2id))\n",
    "        for data in datas:\n",
    "            seg = jieba.cut(data[\"title\"])\n",
    "            raw_docs.append([topic, data[\"title\"]])\n",
    "            docs.append(\" \".join(seg))\n",
    "            labels.append(label2id[topic])\n",
    "    return raw_docs, docs, labels\n",
    "\n",
    "with open(\"dataset/train.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(\"dataset/valid.pkl\", \"rb\") as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "# use baidu stopword set as stopwords\n",
    "stopwords = open(\"stopwords.txt\", \"r\", encoding='utf-8').readlines()\n",
    "stopwords = [i.strip('\\n') for i in stopwords]\n",
    "    \n",
    "train_raw, train_docs, train_labels = split_words(train_data)\n",
    "valid_raw, valid_docs, valid_labels = split_words(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranfer doc to sent\n",
    "def doc2sent(doc):\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            words.remove(word)\n",
    "    return words\n",
    "\n",
    "train_sents = []\n",
    "for doc in train_docs:\n",
    "    train_sents.append(doc2sent(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count frequency\n",
    "def sents2freq(sents):\n",
    "    freq = {}\n",
    "    word_num = 0\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_num += 1\n",
    "            if word in freq.keys():\n",
    "                freq[word] = freq[word] + 1\n",
    "            else:\n",
    "                freq[word] = 1\n",
    "    return freq, word_num\n",
    "\n",
    "word_freq, total_words = sents2freq(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 2 : embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "# skip-gram\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import pickle as gpickle\n",
    "\n",
    "word_model1 = Word2Vec(train_sents, sg=1)\n",
    "gpickle(word_model1, 'word2vec1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector to sentence vector\n",
    "# method 1 : Boolean weighting\n",
    "def wv2sv(sent, word_modelx):\n",
    "    wv_dim = 100\n",
    "    sent_vec = np.zeros(wv_dim)\n",
    "    words_num = 0\n",
    "    for word in sent:\n",
    "        if word in word_modelx.wv:\n",
    "            words_num += 1\n",
    "            sent_vec = sent_vec + np.asarray(word_modelx.wv[word])\n",
    "    if words_num == 0:\n",
    "        return np.zeros(wv_dim)\n",
    "    sent_vec = sent_vec / words_num\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word2vec Skipgram\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import unpickle as upickle\n",
    "from gensim.utils import pickle as gpickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "word_model1 = upickle('word2vec1.pkl')\n",
    "\n",
    "sent_vecs_w2v1 = []\n",
    "for sent in train_sents:\n",
    "    sent_vecs_w2v1.append(wv2sv(sent, word_model1))\n",
    "    \n",
    "sent_out1 = open('sent2vec1.pkl', 'wb')\n",
    "pickle.dump(sent_vecs_w2v1, sent_out1)\n",
    "sent_out1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for valid set\n",
    "def wv2sv_valid(sent, word_model):\n",
    "    wv_dim = 100\n",
    "    sent_vec = np.zeros(wv_dim)\n",
    "    words_num = 0\n",
    "    for word in sent:\n",
    "        if word in word_model.wv:\n",
    "            words_num += 1\n",
    "            sent_vec = sent_vec + np.asarray(word_model.wv[word])\n",
    "    if words_num == 0:\n",
    "        return np.zeros(wv_dim)\n",
    "    sent_vec = sent_vec / words_num\n",
    "    return sent_vec\n",
    "\n",
    "valid_sents = []\n",
    "for doc in valid_docs:\n",
    "    valid_sents.append(doc2sent(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid set word2vec Skipgram \n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "sent_vecs_w2v_valid1 = []\n",
    "for sent in valid_sents:\n",
    "    sent_vecs_w2v_valid1.append(wv2sv_valid(sent, word_model1))\n",
    "\n",
    "sent_out_valid1 = open('sent2vec1v.pkl', 'wb')\n",
    "pickle.dump(sent_vecs_w2v_valid1, sent_out_valid1)\n",
    "sent_out_valid1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentence vector from word2vec\n",
    "# method2 : use SIF to combine word vectors\n",
    "# SIF Smooth Inverse Frequency weighting scheme\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import unpickle as upickle\n",
    "from gensim.utils import pickle as gpickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "\n",
    "def chg_weight(word):\n",
    "    a = 0.0007\n",
    "    if word in word_freq.keys():\n",
    "        p_w = word_freq[word] / total_words\n",
    "    new_weight = a / (a + p_w)\n",
    "    return new_weight\n",
    "\n",
    "def compute_svd(X, nc=1):\n",
    "    svd = TruncatedSVD(n_components=nc, n_iter=10, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_svd(X, nc=1):\n",
    "    svd_components = compute_svd(X, nc)\n",
    "    if nc == 1:\n",
    "        Y = X - X.dot(svd_components.transpose()) * svd_components\n",
    "    else:\n",
    "        Y = X - X.dot(svd_components.transpose()).dot(svd_components)\n",
    "    return Y\n",
    "\n",
    "def wv2sv_optim(sent, word_model):\n",
    "    wv_dim = 100\n",
    "    sent_vec = np.zeros(wv_dim)\n",
    "    words_num = 0\n",
    "    for word in sent:\n",
    "        if word in word_model.wv:\n",
    "            words_num += 1\n",
    "            nw = chg_weight(word)\n",
    "            sent_vec = sent_vec + np.asarray(word_model.wv[word]) * nw\n",
    "    if words_num == 0:\n",
    "        return np.zeros(wv_dim)\n",
    "    sent_vec = sent_vec / words_num\n",
    "    return sent_vec\n",
    "\n",
    "def SIF(sents, word_model):\n",
    "    sv_dim = 100\n",
    "    X = np.zeros((len(sents), sv_dim))\n",
    "    for i in range(len(sents)):\n",
    "        X[i, :] = wv2sv_optim(sents[i], word_model)\n",
    "    X = remove_svd(X)\n",
    "    return X\n",
    "\n",
    "word_model1 = upickle('word2vec1.pkl')\n",
    "\n",
    "sent_vecs_optim = SIF(train_sents, word_model1)\n",
    "sent_vecs_valid_optim = SIF(valid_sents, word_model1)\n",
    "    \n",
    "with open('sent2vec_sif1.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_vecs_optim, f)\n",
    "with open('sent2vec_sif1v.pkl', 'wb') as f:\n",
    "    pickle.dump(sent_vecs_valid_optim, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sentence vector directly from doc\n",
    "# doc2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import pickle as gpickle\n",
    "\n",
    "sents_num = len(train_sents)\n",
    "train_tdocs = [TaggedDocument(train_sents[i], [i]) for i in range(sents_num)]\n",
    "doc_model = Doc2Vec(train_tdocs, window=5)\n",
    "gpickle(doc_model, 'doc2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc2vec\n",
    "# sent vecs for train & valid\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def dv2sv(sent, doc_model):\n",
    "    return doc_model.infer_vector(sent)\n",
    "\n",
    "sent_vecs_d2v = []\n",
    "for sent in train_sents:\n",
    "    sent_vecs_d2v.append(doc_model.infer_vector(sent))\n",
    "    \n",
    "sent_out2 = open('doc2vec_train.pkl', 'wb')\n",
    "pickle.dump(sent_vecs_d2v, sent_out2)\n",
    "sent_out2.close()\n",
    "    \n",
    "sent_vecs_d2v_valid = []\n",
    "for sent in valid_sents:\n",
    "    sent_vecs_d2v_valid.append(doc_model.infer_vector(sent))\n",
    "\n",
    "sent_out_valid2 = open('doc2vec_valid.pkl', 'wb')\n",
    "pickle.dump(sent_vecs_d2v_valid, sent_out_valid2)\n",
    "sent_out_valid2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 3 : classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify by naive bayes / Logistic Regression / SVM\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def predict_label_scale(sent_vecs, sent_vecs_valid, train_labels, train_model):\n",
    "    \n",
    "    trainx = np.asarray(sent_vecs)\n",
    "    validx = np.asarray(sent_vecs_valid)\n",
    "    scalert = MinMaxScaler()\n",
    "    train_x = scalert.fit_transform(trainx)\n",
    "    scalerv = MinMaxScaler()\n",
    "    valid_x = scalerv.fit_transform(validx)\n",
    "    model_nb = train_model()\n",
    "    model_nb.fit(train_x, train_labels)\n",
    "    pred = model_nb.predict(valid_x)\n",
    "    return pred\n",
    "\n",
    "def predict_label(sent_vecs, sent_vecs_valid, train_labels, train_model):\n",
    "    train_x = np.asarray(sent_vecs)\n",
    "    valid_x = np.asarray(sent_vecs_valid)\n",
    "    model_nb = train_model()\n",
    "    model_nb.fit(train_x, train_labels)\n",
    "    pred = model_nb.predict(valid_x)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word2vec Skipgram Booleanw\n",
    "import pickle\n",
    "\n",
    "with open('sent2vec1.pkl', 'rb') as f:\n",
    "    sent_vecs_w2v1 = pickle.load(f)\n",
    "with open('sent2vec1v.pkl', 'rb') as f:\n",
    "    sent_vecs_w2v_valid1 = pickle.load(f)\n",
    "    \n",
    "pred_w2v1 = predict_label(sent_vecs_w2v1, sent_vecs_w2v_valid1, train_labels, LR)\n",
    "print('acc = %.4f' % (sum(pred_w2v1 == valid_labels) / len(valid_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word2vec Skipgram SIF\n",
    "pred_w2v2 = predict_label(sent_vecs_optim, sent_vecs_valid_optim, train_labels, LR)\n",
    "print('acc = %.4f' % (sum(pred_w2v2 == valid_labels) / len(valid_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc2vec\n",
    "import pickle\n",
    "\n",
    "with open('doc2vec_train.pkl', 'rb') as f:\n",
    "    sent_vecs_d2v = pickle.load(f)\n",
    "with open('doc2vec_valid.pkl', 'rb') as f:\n",
    "    sent_vecs_d2v_valid = pickle.load(f)\n",
    "    \n",
    "pred_d2v = predict_label(sent_vecs_d2v, sent_vecs_d2v_valid, train_labels, LR)\n",
    "print('acc = %.4f' % (sum(pred_d2v == valid_labels) / len(valid_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 4 : similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word mover distance\n",
    "# use wmdistance defined in gensim\n",
    "def Find_wmd(query_sent, word_model):\n",
    "    query_seg = doc2sent(query_sent)\n",
    "    score = np.zeros(train_x.shape[0])\n",
    "    for i in range(len(train_sents)):\n",
    "        score[i] = word_model.wmdistance(query_sent, train_sent[i])\n",
    "    ids = list(range(train_x.shape[0]))\n",
    "    ids.sort(key = lambda x: score[x])\n",
    "    for i in ids[20]:\n",
    "        print(train_raw[i][0], train_raw[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "Find_wmd('王者荣耀国际版入选东南亚运动会电竞项目', word_model1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
