{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project 1 level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 1 : BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "label2id = {}\n",
    "def split_words(dataset):\n",
    "    raw_docs = []\n",
    "    docs = []\n",
    "    labels = []\n",
    "    for topic, datas in dataset.items():\n",
    "        if not topic in label2id.keys():\n",
    "            label2id[topic] = len(list(label2id))\n",
    "        for data in datas:\n",
    "            seg = jieba.cut(data[\"title\"])\n",
    "            raw_docs.append([topic, data[\"title\"]])\n",
    "            docs.append(\" \".join(seg))\n",
    "            labels.append(label2id[topic])\n",
    "    return raw_docs, docs, labels\n",
    "\n",
    "with open(\"dataset/train.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(\"dataset/valid.pkl\", \"rb\") as f:\n",
    "    valid_data = pickle.load(f)\n",
    "\n",
    "stopwords = open(\"stopwords.txt\", \"r\", encoding='utf-8').readlines()\n",
    "stopwords = [i.strip('\\n') for i in stopwords]\n",
    "    \n",
    "train_raw, train_docs, train_labels = split_words(train_data)\n",
    "valid_raw, valid_docs, valid_labels = split_words(valid_data)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "bowModel = CountVectorizer(stop_words=stopwords, max_features=1000).fit(train_docs)\n",
    "\n",
    "train_x = bowModel.transform(train_docs)\n",
    "valid_x = bowModel.transform(valid_docs)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_x, train_labels)\n",
    "\n",
    "prediction = model.predict(valid_x)\n",
    "print('acc = %.4f' % (sum(prediction == valid_labels) / len(valid_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 2 : extract 2-word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple process to reduce search space\n",
    "\n",
    "# tranfer doc to sent\n",
    "def doc2sent(doc):\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            words.remove(word)\n",
    "    return words\n",
    "\n",
    "train_sents = []\n",
    "for doc in train_docs:\n",
    "    train_sents.append(doc2sent(doc))\n",
    "\n",
    "# count frequency\n",
    "def sents2freq(sents):\n",
    "    freq = {}\n",
    "    word_num = 0\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            word_num += 1\n",
    "            if word in freq.keys():\n",
    "                freq[word] = freq[word] + 1\n",
    "            else:\n",
    "                freq[word] = 1\n",
    "    return freq, word_num\n",
    "\n",
    "word_freq, total_words = sents2freq(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by freq\n",
    "words_hf = []\n",
    "for word in word_freq.keys():\n",
    "    if word_freq[word] >= 40 and word != '' and word != '，' and word != '？':\n",
    "        words_hf.append(word)\n",
    "\n",
    "def extract_token(sents, window=2):\n",
    "    words_num = len(words_hf)\n",
    "    token2 = np.zeros((words_num, words_num))\n",
    "    for sent in sents:\n",
    "        prev_word = -1\n",
    "        for word in sent:\n",
    "            if word in words_hf:\n",
    "                temp_word = words_hf.index(word)\n",
    "            else:\n",
    "                temp_word = -1\n",
    "            if prev_word != -1 and temp_word != -1:\n",
    "                token2[prev_word, temp_word] += 1\n",
    "            prev_word = temp_word\n",
    "    token2_feature = []\n",
    "    for i in range(words_num):\n",
    "        for j in range(words_num):\n",
    "            if token2[i, j] >= 60:\n",
    "                token2_feature.append((words_hf[i], words_hf[j]))\n",
    "    return token2_feature\n",
    "\n",
    "token2_feature = extract_token(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(mat):\n",
    "    newf_num = len(token2_feature)\n",
    "    sents_num = mat.shape[0]\n",
    "    feats_num = mat.shape[1] + newf_num\n",
    "    maty = np.zeros((sents_num, feats_num))\n",
    "    for i in range(sents_num):\n",
    "        for j in range(mat.shape[1]):\n",
    "            maty[i, j] = mat[i, j]\n",
    "        prev_word = None\n",
    "        for word in train_sents:\n",
    "            if prev_word != None:\n",
    "                if (prev_word, word) in token2_feature:\n",
    "                    nf = token2_feature.index((prev_word, word))\n",
    "                    maty[i, mat.shape[1] + nf] += 1\n",
    "            prev_word = word\n",
    "    return maty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### step 3 : prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = add_feature(train_x)\n",
    "valid_y = add_feature(valid_x)\n",
    "\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(train_y, train_labels)\n",
    "\n",
    "prediction1 = model1.predict(valid_y)\n",
    "print('acc = %.4f' % (sum(prediction1 == valid_labels) / len(valid_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Find(query_str, train_set):\n",
    "    seg = jieba.cut(query_str)\n",
    "    vec = bowModel.transform([\" \".join(seg)])\n",
    "    score = np.zeros(train_set.shape[0])\n",
    "    for i in range(train_set.shape[0]):\n",
    "        diff = np.array(vec) - np.array(train_set[i])\n",
    "        score[i] = abs(diff).sum()\n",
    "    ids = list(range(train_set.shape[0]))\n",
    "    ids.sort(key=lambda x:score[x])\n",
    "    for i in ids[:20]:\n",
    "        print(train_raw[i][0], train_raw[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "Find(\"王者荣耀国际版入选东南亚运动会电竞项目\", train_x)\n",
    "Find(\"王者荣耀国际版入选东南亚运动会电竞项目\", train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
